{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UFESEuEgbUDD"
   },
   "source": [
    "# Interpretation of BertForSequenceClassification in captum\n",
    "\n",
    "In this notebook we'll see how use Captum's Layer Integrated Gradients method to interpret a BERT sentiment classifier that has been finetuned on the imdb dataset https://huggingface.co/lvwerra/bert-imdb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependencies\n",
    "\n",
    "We'll begin by installing library dependencies, namely the `captum` and `transformers` libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 torchaudio===0.7.2 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EJ51JAxHbghp",
    "outputId": "c8c55a99-7e4f-4cdd-b20c-0f2528c4631b"
   },
   "outputs": [],
   "source": [
    "# !pip install transformers==4.1.1\n",
    "# !pip install captum==0.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-31 19:54:10.521998: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers version: 4.1.1\n",
      "Captum version : 0.5.0\n"
     ]
    }
   ],
   "source": [
    "import captum\n",
    "import transformers\n",
    "\n",
    "print(f'Transformers version: {transformers.__version__}')   # 4.1.1\n",
    "print(f'Captum version : {captum.__version__}')              # 0.5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "CS9Kaz8ubUDG"
   },
   "outputs": [],
   "source": [
    "from captum.attr import LayerIntegratedGradients\n",
    "from captum.attr import visualization as viz\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from transformers import BertForSequenceClassification, BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook runs faster when enabled with GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "P1yl1gdvbUDS"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(f'device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download BERT model from HuggingFace\n",
    "\n",
    "We'll use a pretrained BERT model from HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3U5XDt1Gb73t",
    "outputId": "f970b83f-8351-42cf-ad82-3f6df7771e04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-10-31 19:54:37--  https://s3.amazonaws.com/models.huggingface.co/bert/lvwerra/bert-imdb/config.json\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.196.48\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.196.48|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 705 [application/json]\n",
      "Saving to: ‘./model/config.json’\n",
      "\n",
      "config.json         100%[===================>]     705  --.-KB/s    in 0s      \n",
      "\n",
      "2022-10-31 19:54:38 (53.5 MB/s) - ‘./model/config.json’ saved [705/705]\n",
      "\n",
      "--2022-10-31 19:54:38--  https://s3.amazonaws.com/models.huggingface.co/bert/lvwerra/bert-imdb/pytorch_model.bin\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.196.48\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.196.48|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1334420863 (1.2G) [application/octet-stream]\n",
      "Saving to: ‘./model/pytorch_model.bin’\n",
      "\n",
      "pytorch_model.bin   100%[===================>]   1.24G  46.5MB/s    in 28s     \n",
      "\n",
      "2022-10-31 19:55:06 (45.3 MB/s) - ‘./model/pytorch_model.bin’ saved [1334420863/1334420863]\n",
      "\n",
      "--2022-10-31 19:55:07--  https://s3.amazonaws.com/models.huggingface.co/bert/lvwerra/bert-imdb/special_tokens_map.json\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.228.56\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.228.56|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 112 [application/json]\n",
      "Saving to: ‘./model/special_tokens_map.json’\n",
      "\n",
      "special_tokens_map. 100%[===================>]     112  --.-KB/s    in 0s      \n",
      "\n",
      "2022-10-31 19:55:07 (8.37 MB/s) - ‘./model/special_tokens_map.json’ saved [112/112]\n",
      "\n",
      "--2022-10-31 19:55:07--  https://s3.amazonaws.com/models.huggingface.co/bert/lvwerra/bert-imdb/tokenizer_config.json\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.228.56\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.228.56|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 40 [application/json]\n",
      "Saving to: ‘./model/tokenizer_config.json’\n",
      "\n",
      "tokenizer_config.js 100%[===================>]      40  --.-KB/s    in 0s      \n",
      "\n",
      "2022-10-31 19:55:07 (1.26 MB/s) - ‘./model/tokenizer_config.json’ saved [40/40]\n",
      "\n",
      "--2022-10-31 19:55:07--  https://s3.amazonaws.com/models.huggingface.co/bert/lvwerra/bert-imdb/training_args.bin\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.228.56\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.228.56|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1449 (1.4K) [application/octet-stream]\n",
      "Saving to: ‘./model/training_args.bin’\n",
      "\n",
      "training_args.bin   100%[===================>]   1.42K  --.-KB/s    in 0s      \n",
      "\n",
      "2022-10-31 19:55:08 (73.2 MB/s) - ‘./model/training_args.bin’ saved [1449/1449]\n",
      "\n",
      "--2022-10-31 19:55:08--  https://s3.amazonaws.com/models.huggingface.co/bert/lvwerra/bert-imdb/vocab.txt\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.228.56\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.228.56|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 213450 (208K) [text/plain]\n",
      "Saving to: ‘./model/vocab.txt’\n",
      "\n",
      "vocab.txt           100%[===================>] 208.45K  1.10MB/s    in 0.2s    \n",
      "\n",
      "2022-10-31 19:55:08 (1.10 MB/s) - ‘./model/vocab.txt’ saved [213450/213450]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get model and config files from https://huggingface.co/lvwerra/bert-imdb\n",
    "!wget -P ./model https://s3.amazonaws.com/models.huggingface.co/bert/lvwerra/bert-imdb/config.json\n",
    "!wget -P ./model https://s3.amazonaws.com/models.huggingface.co/bert/lvwerra/bert-imdb/pytorch_model.bin\n",
    "!wget -P ./model https://s3.amazonaws.com/models.huggingface.co/bert/lvwerra/bert-imdb/special_tokens_map.json\n",
    "!wget -P ./model https://s3.amazonaws.com/models.huggingface.co/bert/lvwerra/bert-imdb/tokenizer_config.json\n",
    "!wget -P ./model https://s3.amazonaws.com/models.huggingface.co/bert/lvwerra/bert-imdb/training_args.bin\n",
    "!wget -P ./model https://s3.amazonaws.com/models.huggingface.co/bert/lvwerra/bert-imdb/vocab.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll load the model and pre-trained BERT tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "X-nyyq_tbUDa"
   },
   "outputs": [],
   "source": [
    "# Load the model.\n",
    "model = BertForSequenceClassification.from_pretrained('./model')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "model.zero_grad()\n",
    "\n",
    "# Load the pretrained tokenizer.\n",
    "tokenizer = BertTokenizer.from_pretrained('./model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pre-trained BERT tokenizer has special tokens that are used when pre-training the BERT model. \n",
    "BERT is pre-trained with two tasks: A classification task which predicts if one sentence follows another in the original corpus, and a masked language task which predicts which word was masked from a sentence. \n",
    "\n",
    "The BERT tokenizer has special tokens for the \"next sentence prediction\" task. Namely, we need a way to inform the model where does the first sentence end, and where does the second sentence begin. The `SEP` token is used as a separator added to the end of text, and `CLS` is used for prepending the two sentences. \n",
    "\n",
    "The `PAD` token is used to pad sequences to have constant length. \n",
    "\n",
    "The 'CLS' token is used to prepend to the concatenated question-text word sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "SIbauwGbbUDo"
   },
   "outputs": [],
   "source": [
    "ref_token_id = tokenizer.pad_token_id\n",
    "sep_token_id = tokenizer.sep_token_id\n",
    "cls_token_id = tokenizer.cls_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "mcnTCNUFbUD1"
   },
   "outputs": [],
   "source": [
    "def construct_input_ref_pair(text, ref_token_id, sep_token_id, cls_token_id):\n",
    "    text_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "    \n",
    "    # Construct input token ids.\n",
    "    input_ids = [cls_token_id] + text_ids + [sep_token_id]\n",
    "    \n",
    "    # Construct reference token ids \n",
    "    ref_input_ids = [cls_token_id] + [ref_token_id] * len(text_ids) + [sep_token_id]\n",
    "\n",
    "    return torch.tensor([input_ids], device=device), torch.tensor([ref_input_ids], device=device), len(text_ids)\n",
    "\n",
    "def construct_input_ref_token_type_pair(input_ids, sep_ind=0):\n",
    "    seq_len = input_ids.size(1)\n",
    "    token_type_ids = torch.tensor([[0 if i <= sep_ind else 1 for i in range(seq_len)]], device=device)\n",
    "    ref_token_type_ids = torch.zeros_like(token_type_ids, device=device)# * -1\n",
    "    \n",
    "    return token_type_ids, ref_token_type_ids\n",
    "\n",
    "def construct_input_ref_pos_id_pair(input_ids):\n",
    "    seq_length = input_ids.size(1)\n",
    "    position_ids = torch.arange(seq_length, dtype=torch.long, device=device)\n",
    "    ref_position_ids = torch.zeros(seq_length, dtype=torch.long, device=device)\n",
    "\n",
    "    position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "    ref_position_ids = ref_position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "    \n",
    "    return position_ids, ref_position_ids\n",
    "    \n",
    "def construct_attention_mask(input_ids):\n",
    "    return torch.ones_like(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "vhasPia4bUD8"
   },
   "outputs": [],
   "source": [
    "def custom_forward(inputs):\n",
    "    preds = model(inputs)[0]\n",
    "    return torch.softmax(preds, dim = 1)[0][1].unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at an example prediction for the input text: \"If you like the original, you'll love this movie.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"If you like the original, you'll love this movie.\"\n",
    "\n",
    "input_ids, ref_input_ids, sep_id = construct_input_ref_pair(sample_text, ref_token_id, sep_token_id, cls_token_id)\n",
    "token_type_ids, ref_token_type_ids = construct_input_ref_token_type_pair(input_ids, sep_id)\n",
    "position_ids, ref_position_ids = construct_input_ref_pos_id_pair(input_ids)\n",
    "attention_mask = construct_attention_mask(input_ids)\n",
    "\n",
    "indices = input_ids[0].detach().tolist()\n",
    "all_tokens = tokenizer.convert_ids_to_tokens(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized text: ['[CLS]', 'If', 'you', 'like', 'the', 'original', ',', 'you', \"'\", 'll', 'love', 'this', 'movie', '.', '[SEP]']\n",
      "text as indices: [101, 1409, 1128, 1176, 1103, 1560, 117, 1128, 112, 1325, 1567, 1142, 2523, 119, 102]\n"
     ]
    }
   ],
   "source": [
    "print(f'tokenized text: {all_tokens}')\n",
    "print(f'text as indices: {indices}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model predict: tensor([[-3.2322,  3.4528]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "custom forward pass: tensor([0.9988], device='cuda:0', grad_fn=<UnsqueezeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Check predict output.\n",
    "print(f'model predict: {model(input_ids)[0]}')\n",
    "\n",
    "# Predict using custom forward pass.\n",
    "print(f'custom forward pass: {custom_forward(input_ids)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpret model predicitons using Captum's Layer Integrated Gradients method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Layer Integrated Gradients object and compute attributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "pGwkb1vAbUEA"
   },
   "outputs": [],
   "source": [
    "lig = LayerIntegratedGradients(custom_forward, model.bert.embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "YAzBqQlpbUEk"
   },
   "outputs": [],
   "source": [
    "lig_attributions, delta = lig.attribute(inputs=input_ids,\n",
    "                                        baselines=ref_input_ids,\n",
    "                                        n_steps=700,\n",
    "                                        internal_batch_size=3,\n",
    "                                        return_convergence_delta=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input example: If you like the original, you'll love this movie.\n",
      "Sentiment:  1, Probability positive: 0.9987521\n"
     ]
    }
   ],
   "source": [
    "predict_sentiment = torch.argmax(model(input_ids)[0]).cpu().numpy()\n",
    "positive_prob = custom_forward(input_ids).detach().cpu().numpy()\n",
    "\n",
    "print(f'Input example: {sample_text}')\n",
    "print(f'Sentiment:  {str(predict_sentiment)}, Probability positive: {str(positive_prob[0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Hq8R_ZYubUEu"
   },
   "outputs": [],
   "source": [
    "def summarize_attributions(attributions):\n",
    "    attributions = attributions.sum(dim=-1).squeeze(0)\n",
    "    attributions = attributions / torch.norm(attributions)\n",
    "    return attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "3q7xXwRrbUEx"
   },
   "outputs": [],
   "source": [
    "attributions_sum = summarize_attributions(lig_attributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "0ZF0RmZ4bUE1"
   },
   "outputs": [],
   "source": [
    "# storing couple samples in an array for visualization purposes\n",
    "score_vis = viz.VisualizationDataRecord(word_attributions=attributions_sum,\n",
    "                                        pred_prob=torch.softmax(model(input_ids)[0], dim = 1)[0][1],\n",
    "                                        pred_class=torch.argmax(model(input_ids)[0]),\n",
    "                                        true_class=1,\n",
    "                                        attr_class=sample_text,\n",
    "                                        attr_score=attributions_sum.sum(),       \n",
    "                                        raw_input_ids=all_tokens,\n",
    "                                        convergence_score=delta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 199
    },
    "id": "-gAojuO6ody0",
    "outputId": "abebee9c-a2b6-429d-9725-ed9166e25be6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>1</b></text></td><td><text style=\"padding-right:2em\"><b>1 (1.00)</b></text></td><td><text style=\"padding-right:2em\"><b>If you like the original, you'll love this movie.</b></text></td><td><text style=\"padding-right:2em\"><b>1.04</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 85%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> If                    </font></mark><mark style=\"background-color: hsl(0, 75%, 77%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> you                    </font></mark><mark style=\"background-color: hsl(0, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> like                    </font></mark><mark style=\"background-color: hsl(120, 75%, 81%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> original                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> you                    </font></mark><mark style=\"background-color: hsl(120, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> '                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ll                    </font></mark><mark style=\"background-color: hsl(120, 75%, 85%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> love                    </font></mark><mark style=\"background-color: hsl(120, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> this                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> movie                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "viz.visualize_text([score_vis])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The negative attribution for 'you like' goes against our intuition, but the positive attibution for \"love\" seems to match what we would expect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2022 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Interpretation.ipynb",
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-3.m99",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m99"
  },
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
