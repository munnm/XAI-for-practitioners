{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20dfc54c-b9ca-4cb0-839f-ced7e5308bda",
   "metadata": {},
   "source": [
    "# Layer-wise Relevance Propagation\n",
    "\n",
    "This notebook is adapted from the [BERT-explainability notebook](https://colab.sandbox.google.com/github/hila-chefer/Transformer-Explainability/blob/main/BERT_explainability.ipynb) of Hila Cifar et al. and their work [Transformer Interpretability Beyond Attention Visualization](https://arxiv.org/abs/2012.09838) published in [CVPR 2021](https://cvpr2021.thecvf.com/).\n",
    "\n",
    "**using a GPU will speed up performance** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317b9cb6-f0ba-4e0d-8da8-532f8b0c4c84",
   "metadata": {},
   "source": [
    "Start by cloning the repo from the official (pytorch) implementation of the paper `Transformer Interpretability Beyond Attention Visualization`.\n",
    "This paper introduces a novel method for visualizing classifications made by a Transformer based model for NLP (as well as vision) tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8572b9f5-3c93-4979-8883-6631c624ac52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/hila-chefer/Transformer-Explainability.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f0e474-8bba-4e17-82f3-fe7e945b8907",
   "metadata": {},
   "source": [
    "Next, install the additional requirements for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8eb9da9-ba70-4919-bd3a-1d356e6180fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --user -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f079b635-33b3-4796-ac0a-f698493e7549",
   "metadata": {},
   "source": [
    "#### Restart the kernel\n",
    "\n",
    "After you install the additional packages, you need to restart the notebook kernel so it can find the packages.\n",
    "Next, we'll import the necessary pacakages and libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "225311d9-33d8-4460-80d2-055751856222",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('./Transformer-Explainability')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd9e59aa-94b1-4573-8b35-35c81bea2adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-21 04:51:35.313794: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "from BERT_explainability.modules.BERT.BertForSequenceClassification import BertForSequenceClassification\n",
    "from BERT_explainability.modules.BERT.ExplanationGenerator import Generator\n",
    "from captum.attr import visualization\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fc33465-d79f-48f8-af04-f3a22839612a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using GPU...\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"using GPU...\")\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        \"textattack/bert-base-uncased-SST-2\").to(\"cuda\")\n",
    "else:\n",
    "    print(\"...not using GPU...\")\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        \"textattack/bert-base-uncased-SST-2\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"textattack/bert-base-uncased-SST-2\")\n",
    "\n",
    "# initialize the explanations generator\n",
    "explanations = Generator(model)\n",
    "\n",
    "classifications = [\"NEGATIVE\", \"POSITIVE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8aea716-e55e-4d9a-a650-1c60de232243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using GPU...\n"
     ]
    }
   ],
   "source": [
    "# encode a sentence\n",
    "text_batch = [\"If you liked the original, you'll love this movie.\"]\n",
    "encoding = tokenizer(text_batch, return_tensors='pt')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"using GPU...\")\n",
    "    input_ids = encoding['input_ids'].to(\"cuda\")\n",
    "    attention_mask = encoding['attention_mask'].to(\"cuda\")\n",
    "else:\n",
    "    print(\"not using GPU...\")\n",
    "    input_ids = encoding[\"input_ids\"]\n",
    "    attention_mask = encoding[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35345ff1-762d-449a-b139-1a5c593be878",
   "metadata": {},
   "source": [
    "Here `encoding` is a transformer tokenization containing `input_ids`, `token_type_ids`, and `attention_mask`. We'll pull these out to supply them to the LRP explanation method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58eeb552-ec17-4374-8cc7-1f245669d8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# true class is positive - 1\n",
    "true_class = 1\n",
    "\n",
    "# generate an explanation for the input\n",
    "lrp_expl = explanations.generate_LRP(input_ids=input_ids,\n",
    "                                     attention_mask=attention_mask,\n",
    "                                     start_layer=0)[0]\n",
    "\n",
    "# normalize scores\n",
    "lrp_expl = (lrp_expl - lrp_expl.min()) / (lrp_expl.max() - lrp_expl.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d20a1680-9f3d-4582-b239-864eaf35798e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model classification.\n",
    "output = torch.nn.functional.softmax(model(input_ids=input_ids,\n",
    "                                           attention_mask=attention_mask)[0],\n",
    "                                     dim=-1)\n",
    "classification = output.argmax(dim=-1).item()\n",
    "\n",
    "# Get class name.\n",
    "class_name = classifications[classification]\n",
    "\n",
    "# If the classification is negative, higher explanation scores are more\n",
    "# negative, so flip for visualization.\n",
    "if class_name == \"NEGATIVE\":\n",
    "    lrp_expl *= (-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be8211b6-83e3-4498-86d6-57dd72041dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([CLS]: 0.0)\n",
      "(if: 0.06772662699222565)\n",
      "(you: 0.18428470194339752)\n",
      "(liked: 0.11215817928314209)\n",
      "(the: 0.02981744147837162)\n",
      "(original: 0.03102767840027809)\n",
      "(,: 0.011994951404631138)\n",
      "(you: 0.20723497867584229)\n",
      "(': 0.0)\n",
      "(ll: 0.40791115164756775)\n",
      "(love: 1.0)\n",
      "(this: 0.4568018913269043)\n",
      "(movie: 0.09963283687829971)\n",
      "(.: 0.0043361312709748745)\n",
      "([SEP]: 0.018694186583161354)\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(input_ids.flatten())\n",
    "\n",
    "for i in range(len(tokens)):\n",
    "    print(f'({tokens[i]}: {lrp_expl[i].item()})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5464844-ff24-4cd3-9876-01b8eed77061",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>1</b></text></td><td><text style=\"padding-right:2em\"><b>1 (1.00)</b></text></td><td><text style=\"padding-right:2em\"><b>1</b></text></td><td><text style=\"padding-right:2em\"><b>1.00</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> if                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> you                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> liked                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> original                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> you                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> '                    </font></mark><mark style=\"background-color: hsl(120, 75%, 80%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ll                    </font></mark><mark style=\"background-color: hsl(120, 75%, 50%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> love                    </font></mark><mark style=\"background-color: hsl(120, 75%, 78%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> this                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> movie                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vis_data_records = [\n",
    "    visualization.VisualizationDataRecord(word_attributions=lrp_expl,\n",
    "                                          pred_prob=output[0][classification],\n",
    "                                          pred_class=classification,\n",
    "                                          true_class=true_class,\n",
    "                                          attr_class=true_class,\n",
    "                                          attr_score=1,\n",
    "                                          raw_input_ids=tokens,\n",
    "                                          convergence_score=1)]\n",
    "\n",
    "visualization.visualize_text(vis_data_records)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdfd31e-eada-4d4c-8d20-78a419aff778",
   "metadata": {},
   "source": [
    "Copyright 2022 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-3.m95",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m95"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
